{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Household</td>\n",
       "      <td>Paper Plane Design Framed Wall Hanging Motivat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Household</td>\n",
       "      <td>SAF 'Floral' Framed Painting (Wood, 30 inch x ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                            feature\n",
       "0  Household  Paper Plane Design Framed Wall Hanging Motivat...\n",
       "1  Household  SAF 'Floral' Framed Painting (Wood, 30 inch x ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../datasets/ecommerceDataset.csv', names =['target', 'feature'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we preprocess the data.\n",
    "\n",
    "- convert_to_lowercase:  converts the text to lowercase \n",
    "- remove_whitespaces: removes unnecessary empty whitespaces from the text \n",
    "- remove_punctuations: removes punctuations but we keep the apostrophes \n",
    "- remove_html: removes html links from the text \n",
    "- remove_http: removes http links from the text \n",
    "- remove_stopwords: removing stop words since they have no impact on the classification procedure \n",
    "- text_stemmer: converting the words to their root form \n",
    "- discard_non_alpha: discarding non-alphabetic words because they create unnecessary diversions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we do Data Preprocessing.\n",
    "\"\"\"\n",
    "import string, re, nltk\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "def remove_whitespace(text):\n",
    "    return text.strip()\n",
    "def remove_punctuation(text):\n",
    "    punct_str = string.punctuation\n",
    "    punct_str = punct_str.replace(\"'\", \"\") \n",
    "    return text.translate(str.maketrans(\"\", \"\", punct_str))\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "def remove_http(text):\n",
    "    http = \"https?://\\S+|www\\.\\S+\" \n",
    "    pattern = r\"({})\".format(http) \n",
    "    return re.sub(pattern, \"\", text)\n",
    "# Stopwords\n",
    "stops = stopwords.words(\"english\") \n",
    "addstops = [\"among\", \"onto\", \"shall\", \"thrice\", \"thus\", \"twice\", \"unto\", \"us\", \"would\"] \n",
    "allstops = stops + addstops\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in regexp.tokenize(text) if word not in allstops])\n",
    "stemmer = PorterStemmer()\n",
    "def text_stemmer(text):\n",
    "    text_stem = \" \".join([stemmer.stem(word) for word in regexp.tokenize(text)])\n",
    "    return text_stem\n",
    "def discard_non_alpha(text):\n",
    "    word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n",
    "    text_non_alpha = \" \".join(word_list_non_alpha)\n",
    "    return text_non_alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration process \n",
    "\n",
    "- We integrate the text normalization processes in appropriate order. We also converted the text into one line and removed square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalizer(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = re.sub('\\n' , '', text)\n",
    "    text = re.sub('\\[.*?\\]', '', text) \n",
    "    text = remove_http(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = discard_non_alpha(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paper plane design framed wall hanging motivational office decor art prints x inch set painting made synthetic frame uv textured print gives multi effects attracts towards special series paintings makes wall beautiful gives royal touch painting ready hang proud possess unique painting niche apart use modern efficient printing technology prints inks precision epson roland hp printers innovative hd printing technique results durable spectacular looking prints highest last lifetime print solely topnotch inks achieve brilliant true colours due high level uv resistance prints retain beautiful colours many years add colour style living space digitally printed painting pleasure eternal blissso bring home elegant print lushed rich colors makes nothing sheer elegance friends familyit treasured forever whoever lucky recipient liven place intriguing paintings high definition hd graphic digital prints home office room'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['feature'] = df['feature'].apply(text_normalizer)\n",
    "df['feature'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the compression ratio $CR$\n",
    "\n",
    "- method calculate_compression_ratio:\n",
    "  param text: text that is used to calculate the compression ratio\n",
    "  - the compression ratio is computed by: original encoded text length devided by the gzip compressed text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression Ratio - Train Set: 2.8164242110043336\n",
      "Compression Ratio - Test Set: 2.820859067511082\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "# Calculate compression ratio\n",
    "def calculate_compression_ratio(text):\n",
    "    compressed = len(gzip.compress(text.encode()))\n",
    "    original = len(text.encode())\n",
    "    compression_ratio = original / compressed\n",
    "    return compression_ratio\n",
    "\n",
    "# Print compression ratios for train and test sets\n",
    "train_compression_ratio = calculate_compression_ratio(\" \".join(df_train['feature']))\n",
    "test_compression_ratio = calculate_compression_ratio(\" \".join(df_test['feature']))\n",
    "\n",
    "print(\"Compression Ratio - Train Set:\", train_compression_ratio)\n",
    "print(\"Compression Ratio - Test Set:\", test_compression_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating numerical characteristics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N - Number of training examples: 40339\n",
      "N - Number of test examples: 10085\n",
      "C - Number of classes: 4\n",
      "W - Average number of words in each example (Train): 70.23545452291827\n",
      "W - Average number of words in each example (Test): 70.15815567674764\n",
      "L - Average number of characters in each example (Train): 513.0067428543098\n",
      "L - Average number of characters in each example (Test): 511.7854238968765\n",
      "V - Vocabulary size: 86662\n"
     ]
    }
   ],
   "source": [
    "#Statistical data about the train and test sets\n",
    "# N - Number of training and test set examples\n",
    "N_train = len(df_train)\n",
    "N_test = len(df_test)\n",
    "\n",
    "# C - Number of classes\n",
    "C = df['target'].nunique()\n",
    "\n",
    "#Calculate average number of words (W) and characters (L) in each example\n",
    "#text is tokenized by spaces\n",
    "train_word_counts = df_train['feature'].apply(lambda x: len(x.split()))\n",
    "test_word_counts = df_test['feature'].apply(lambda x: len(x.split()))\n",
    "train_char_counts = df_train['feature'].apply(lambda x: len(x))\n",
    "test_char_counts = df_test['feature'].apply(lambda x: len(x))\n",
    "W_train = train_word_counts.mean()\n",
    "W_test = test_word_counts.mean()\n",
    "L_train = train_char_counts.mean()\n",
    "L_test = test_char_counts.mean()\n",
    "\n",
    "# V - Vocabulary size\n",
    "all_text = \" \".join(df['feature'])\n",
    "vocabulary = set(all_text.split())\n",
    "V = len(vocabulary)\n",
    "\n",
    "# Print the statistics\n",
    "print(\"N - Number of training examples:\", N_train)\n",
    "print(\"N - Number of test examples:\", N_test)\n",
    "print(\"C - Number of classes:\", C)\n",
    "print(\"W - Average number of words in each example (Train):\", W_train)\n",
    "print(\"W - Average number of words in each example (Test):\", W_test)\n",
    "print(\"L - Average number of characters in each example (Train):\", L_train)\n",
    "print(\"L - Average number of characters in each example (Test):\", L_test)\n",
    "print(\"V - Vocabulary size:\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied gzip compressor-based text classification on the dataset\n",
    "\n",
    "- for each compressed test set record, join with compressed training record & compute the distance between compressed test record and concatenated train + test record\n",
    "- $kNN$ majority vote at the end (get most freuquent class among top k neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 1091/10085 [1:50:43<15:12:46,  6.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m c_test_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gzip\u001b[38;5;241m.\u001b[39mcompress(test_text\u001b[38;5;241m.\u001b[39mencode()))\n\u001b[1;32m     12\u001b[0m distance_from_test_instance \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_train \u001b[38;5;129;01min\u001b[39;00m df_train\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     15\u001b[0m     train_text \u001b[38;5;241m=\u001b[39m row_train[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m     train_label \u001b[38;5;241m=\u001b[39m row_train[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:1324\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py?line=1321'>1322</a>\u001b[0m klass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_sliced\n\u001b[1;32m   <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py?line=1322'>1323</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues):\n\u001b[0;32m-> <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py?line=1323'>1324</a>\u001b[0m     s \u001b[39m=\u001b[39m klass(v, index\u001b[39m=\u001b[39;49mcolumns, name\u001b[39m=\u001b[39;49mk)\n\u001b[1;32m   <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py?line=1324'>1325</a>\u001b[0m     \u001b[39myield\u001b[39;00m k, s\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:455\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py?line=452'>453</a>\u001b[0m manager \u001b[39m=\u001b[39m get_option(\u001b[39m\"\u001b[39m\u001b[39mmode.data_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py?line=453'>454</a>\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblock\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py?line=454'>455</a>\u001b[0m     data \u001b[39m=\u001b[39m SingleBlockManager\u001b[39m.\u001b[39;49mfrom_array(data, index)\n\u001b[1;32m    <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py?line=455'>456</a>\u001b[0m \u001b[39melif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py?line=456'>457</a>\u001b[0m     data \u001b[39m=\u001b[39m SingleArrayManager\u001b[39m.\u001b[39mfrom_array(data, index)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:1750\u001b[0m, in \u001b[0;36mSingleBlockManager.from_array\u001b[0;34m(cls, array, index)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=1744'>1745</a>\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=1745'>1746</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_array\u001b[39m(\u001b[39mcls\u001b[39m, array: ArrayLike, index: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SingleBlockManager:\n\u001b[1;32m   <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=1746'>1747</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=1747'>1748</a>\u001b[0m \u001b[39m    Constructor for if we have an array that is not yet a Block.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=1748'>1749</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=1749'>1750</a>\u001b[0m     block \u001b[39m=\u001b[39m new_block(array, placement\u001b[39m=\u001b[39m\u001b[39mslice\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39mlen\u001b[39;49m(index)), ndim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   <a href='file:///Users/makarwuckert/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=1750'>1751</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(block, index)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#normale Gzip-Klassifikationsvariante\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "k = 2\n",
    "\n",
    "predicted_classes = []\n",
    "\n",
    "for row_test in tqdm(df_test.iterrows(), total=df_test.shape[0]):\n",
    "    test_text = row_test[1][\"feature\"]\n",
    "    test_label = row_test[1][\"target\"]\n",
    "    c_test_text = len(gzip.compress(test_text.encode()))\n",
    "    distance_from_test_instance = []\n",
    "    \n",
    "    for row_train in df_train.iterrows():\n",
    "        train_text = row_train[1][\"feature\"]\n",
    "        train_label = row_train[1][\"target\"]\n",
    "        c_train_text = len(gzip.compress(train_text.encode()))\n",
    "        \n",
    "        train_plus_test = \" \".join([test_text, train_text])\n",
    "        c_train_plus_test = len(gzip.compress(train_plus_test.encode()))\n",
    "        \n",
    "        ncd = ( (c_train_plus_test - min(c_train_text, c_test_text))\n",
    "                / max(c_test_text, c_train_text) )\n",
    "        distance_from_test_instance.append(ncd)\n",
    "        \n",
    "    sorted_idx = np.argsort(np.array(distance_from_test_instance))\n",
    "    \n",
    "    top_k_class = list(df_train.iloc[sorted_idx[:k]][\"label\"].values)\n",
    "    predicted_class = max(set(top_k_class), key=top_k_class.count)\n",
    "    #top_k_class = df_train.iloc[sorted_idx[:k]][\"target\"].values\n",
    "    #predicted_class = np.argmax(np.bincount(top_k_class))\n",
    "    \n",
    "    predicted_classes.append(predicted_class)\n",
    "     \n",
    "print(\"Accuracy:\", np.mean(np.array(predicted_classes) == df_test[\"target\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied the tie-breaking-fix (tbf) variant of the gzip compressor based text classification method\n",
    "\n",
    "- we improved tie-breaking using a Counter which selects the first label in case of a tie. If the labels are sorted by the distance we ensure it's picking the closest neighbor in case of a tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#with Tie-breaking fix (MORE ACCURACY)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "k = 2\n",
    "\n",
    "predicted_classes = []\n",
    "\n",
    "for row_test in tqdm(df_test.iterrows(), total=df_test.shape[0]):\n",
    "    test_text = row_test[1][\"feature\"]\n",
    "    test_label = row_test[1][\"target\"]\n",
    "    c_test_text = len(gzip.compress(test_text.encode()))\n",
    "    distance_from_test_instance = []\n",
    "    \n",
    "    for row_train in df_train.iterrows():\n",
    "        train_text = row_train[1][\"feature\"]\n",
    "        train_label = row_train[1][\"target\"]\n",
    "        c_train_text = len(gzip.compress(train_text.encode()))\n",
    "        \n",
    "        train_plus_test = \" \".join([test_text, train_text])\n",
    "        c_train_plus_test = len(gzip.compress(train_plus_test.encode()))\n",
    "        \n",
    "        ncd = ( (c_train_plus_test - min(c_train_text, c_test_text))\n",
    "                / max(c_test_text, c_train_text) )\n",
    "        distance_from_test_instance.append(ncd)\n",
    "        \n",
    "    sorted_idx = np.argsort(np.array(distance_from_test_instance))\n",
    "    top_k_class = np.array(df_train[\"target\"])[sorted_idx[:k]]\n",
    "    predicted_class = Counter(top_k_class).most_common()[0][0]\n",
    "    \n",
    "    predicted_classes.append(predicted_class)\n",
    "        \n",
    "print(\"Accuracy:\", np.mean(np.array(predicted_classes) == df_test[\"target\"].values))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "#true labels for the test set\n",
    "true_labels = df_test['target']\n",
    "\n",
    "# Compute the classification report\n",
    "classification_report_output = classification_report(true_labels, predicted_classes)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report_output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b60f5bfde2dc6003c52249e7ba87cb8994d8effec2917dd9467812f9cc41ae70"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
