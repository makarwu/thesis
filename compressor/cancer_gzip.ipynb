{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Thyroid surgery in  children in a single insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>\" The adopted strategy was the same as that us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>coronary arterybypass grafting thrombosis ï¬b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               0  \\\n",
       "0           0  Thyroid_Cancer   \n",
       "1           1  Thyroid_Cancer   \n",
       "2           2  Thyroid_Cancer   \n",
       "\n",
       "                                                   a  \n",
       "0  Thyroid surgery in  children in a single insti...  \n",
       "1  \" The adopted strategy was the same as that us...  \n",
       "2  coronary arterybypass grafting thrombosis ï¬b...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../datasets/alldata_1_for_kaggle.csv', encoding=\"latin-1\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Thyroid surgery in  children in a single insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>\" The adopted strategy was the same as that us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>coronary arterybypass grafting thrombosis ï¬b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           target                                            feature\n",
       "0  Thyroid_Cancer  Thyroid surgery in  children in a single insti...\n",
       "1  Thyroid_Cancer  \" The adopted strategy was the same as that us...\n",
       "2  Thyroid_Cancer  coronary arterybypass grafting thrombosis ï¬b..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Unnamed: 0', axis = 1)\n",
    "df = df.rename({'0': 'target', 'a': 'feature'}, axis = 1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we do Data Preprocessing.\n",
    "\"\"\"\n",
    "import string, re, nltk\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "def remove_whitespace(text):\n",
    "    return text.strip()\n",
    "def remove_punctuation(text):\n",
    "    punct_str = string.punctuation\n",
    "    punct_str = punct_str.replace(\"'\", \"\") \n",
    "    return text.translate(str.maketrans(\"\", \"\", punct_str))\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "def remove_http(text):\n",
    "    http = \"https?://\\S+|www\\.\\S+\" \n",
    "    pattern = r\"({})\".format(http) \n",
    "    return re.sub(pattern, \"\", text)\n",
    "# Stopwords\n",
    "stops = stopwords.words(\"english\") \n",
    "addstops = [\"among\", \"onto\", \"shall\", \"thrice\", \"thus\", \"twice\", \"unto\", \"us\", \"would\"] \n",
    "allstops = stops + addstops\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in regexp.tokenize(text) if word not in allstops])\n",
    "stemmer = PorterStemmer()\n",
    "def text_stemmer(text):\n",
    "    text_stem = \" \".join([stemmer.stem(word) for word in regexp.tokenize(text)])\n",
    "    return text_stem\n",
    "def discard_non_alpha(text):\n",
    "    word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n",
    "    text_non_alpha = \" \".join(word_list_non_alpha)\n",
    "    return text_non_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalizer(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = re.sub('\\n' , '', text) \n",
    "    text = re.sub('\\[.*?\\]', '', text) \n",
    "    text = remove_http(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = discard_non_alpha(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature'] = df['feature'].apply(text_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression Ratio - Train Set: 3.397251619353535\n",
      "Compression Ratio - Test Set: 3.401567560866739\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "# Calculate compression ratio\n",
    "def calculate_compression_ratio(text):\n",
    "    compressed = len(gzip.compress(text.encode()))\n",
    "    original = len(text.encode())\n",
    "    compression_ratio = original / compressed\n",
    "    return compression_ratio\n",
    "\n",
    "# Print compression ratios for train and test sets\n",
    "train_compression_ratio = calculate_compression_ratio(\" \".join(df_train['feature']))\n",
    "test_compression_ratio = calculate_compression_ratio(\" \".join(df_test['feature']))\n",
    "\n",
    "print(\"Compression Ratio - Train Set:\", train_compression_ratio)\n",
    "print(\"Compression Ratio - Test Set:\", test_compression_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N - Number of training examples: 6056\n",
      "N - Number of test examples: 1514\n",
      "C - Number of classes: 3\n",
      "W - Average number of words in each example (Train): 2180.092470277411\n",
      "W - Average number of words in each example (Test): 2203.9101717305152\n",
      "L - Average number of characters in each example (Train): 18438.607661822985\n",
      "L - Average number of characters in each example (Test): 18681.858652575957\n",
      "V - Vocabulary size: 157912\n"
     ]
    }
   ],
   "source": [
    "#Statistical data about the train and test sets\n",
    "# N - Number of training and test set examples\n",
    "N_train = len(df_train)\n",
    "N_test = len(df_test)\n",
    "\n",
    "# C - Number of classes\n",
    "C = df['target'].nunique()\n",
    "\n",
    "# Calculate average number of words (W) and characters (L) in each example\n",
    "#text is tokenized by spaces\n",
    "train_word_counts = df_train['feature'].apply(lambda x: len(x.split()))\n",
    "test_word_counts = df_test['feature'].apply(lambda x: len(x.split()))\n",
    "train_char_counts = df_train['feature'].apply(lambda x: len(x))\n",
    "test_char_counts = df_test['feature'].apply(lambda x: len(x))\n",
    "W_train = train_word_counts.mean()\n",
    "W_test = test_word_counts.mean()\n",
    "L_train = train_char_counts.mean()\n",
    "L_test = test_char_counts.mean()\n",
    "\n",
    "# V - Vocabulary size\n",
    "# count unique words in the entire dataset\n",
    "all_text = \" \".join(df['feature'])\n",
    "vocabulary = set(all_text.split())\n",
    "V = len(vocabulary)\n",
    "\n",
    "# Print the statistics\n",
    "print(\"N - Number of training examples:\", N_train)\n",
    "print(\"N - Number of test examples:\", N_test)\n",
    "print(\"C - Number of classes:\", C)\n",
    "print(\"W - Average number of words in each example (Train):\", W_train)\n",
    "print(\"W - Average number of words in each example (Test):\", W_test)\n",
    "print(\"L - Average number of characters in each example (Train):\", L_train)\n",
    "print(\"L - Average number of characters in each example (Test):\", L_test)\n",
    "print(\"V - Vocabulary size:\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "#calculate normalized compression distance (NCD)\n",
    "def calculate_ncd(x1, x2):\n",
    "    Cx1 = len(gzip.compress(x1.encode()))\n",
    "    Cx2 = len(gzip.compress(x2.encode()))\n",
    "    x1x2 = \" \".join([x1, x2])\n",
    "    Cx1x2 = len(gzip.compress(x1x2.encode()))\n",
    "    \n",
    "    ncd = (Cx1x2 - min(Cx1, Cx2)) / max(Cx1, Cx2)\n",
    "    return ncd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1514 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wn/6ytm2bz55w14d5rrpd1mctsw0000gn/T/ipykernel_3496/2944275346.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mpredicted_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m            \u001b[0mto\u001b[0m \u001b[0mit\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mno\u001b[0m \u001b[0meffect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \"\"\"\n\u001b[1;32m   1321\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#normale Gzip-Klassifikationsvariante\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "k = 2\n",
    "\n",
    "predicted_classes = []\n",
    "\n",
    "for row_test in tqdm(df_test.iterrows(), total=df_test.shape[0]):\n",
    "    test_text = row_test[1][\"feature\"]\n",
    "    test_label = row_test[1][\"target\"]\n",
    "    c_test_text = len(gzip.compress(test_text.encode()))\n",
    "    distance_from_test_instance = []\n",
    "    \n",
    "    for row_train in df_train.iterrows():\n",
    "        train_text = row_train[1][\"feature\"]\n",
    "        train_label = row_train[1][\"target\"]\n",
    "        c_train_text = len(gzip.compress(train_text.encode()))\n",
    "        \n",
    "        train_plus_test = \" \".join([test_text, train_text])\n",
    "        c_train_plus_test = len(gzip.compress(train_plus_test.encode()))\n",
    "        \n",
    "        ncd = ( (c_train_plus_test - min(c_train_text, c_test_text))\n",
    "                / max(c_test_text, c_train_text) )\n",
    "        distance_from_test_instance.append(ncd)\n",
    "        \n",
    "    sorted_idx = np.argsort(np.array(distance_from_test_instance))\n",
    "    \n",
    "    top_k_class = list(df_train.iloc[sorted_idx[:k]][\"target\"].values)\n",
    "    predicted_class = max(set(top_k_class), key=top_k_class.count)\n",
    "    #top_k_class = df_train.iloc[sorted_idx[:k]][\"target\"].values\n",
    "    #predicted_class = np.argmax(np.bincount(top_k_class))\n",
    "    \n",
    "    predicted_classes.append(predicted_class)\n",
    "     \n",
    "print(\"Accuracy:\", np.mean(np.array(predicted_classes) == df_test[\"target\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#with Tie-breaking fix (MORE ACCURACY)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "k = 2\n",
    "\n",
    "predicted_classes = []\n",
    "\n",
    "for row_test in tqdm(df_test.iterrows(), total=df_test.shape[0]):\n",
    "    test_text = row_test[1][\"feature\"]\n",
    "    test_label = row_test[1][\"target\"]\n",
    "    c_test_text = len(gzip.compress(test_text.encode()))\n",
    "    distance_from_test_instance = []\n",
    "    \n",
    "    for row_train in df_train.iterrows():\n",
    "        train_text = row_train[1][\"feature\"]\n",
    "        train_label = row_train[1][\"target\"]\n",
    "        c_train_text = len(gzip.compress(train_text.encode()))\n",
    "        \n",
    "        train_plus_test = \" \".join([test_text, train_text])\n",
    "        c_train_plus_test = len(gzip.compress(train_plus_test.encode()))\n",
    "        \n",
    "        ncd = ( (c_train_plus_test - min(c_train_text, c_test_text))\n",
    "                / max(c_test_text, c_train_text) )\n",
    "        distance_from_test_instance.append(ncd)\n",
    "        \n",
    "    sorted_idx = np.argsort(np.array(distance_from_test_instance))\n",
    "    top_k_class = np.array(df_train[\"target\"])[sorted_idx[:k]]\n",
    "    predicted_class = Counter(top_k_class).most_common()[0][0]\n",
    "    \n",
    "    predicted_classes.append(predicted_class)\n",
    "        \n",
    "print(\"Accuracy:\", np.mean(np.array(predicted_classes) == df_test[\"target\"].values))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "#true labels for the test set\n",
    "true_labels = df_test['target']\n",
    "\n",
    "# Compute the classification report\n",
    "classification_report_output = classification_report(true_labels, predicted_classes)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report_output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b60f5bfde2dc6003c52249e7ba87cb8994d8effec2917dd9467812f9cc41ae70"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
